{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Pulling without specifying how to reconcile divergent branches is\u001b[m\n",
      "\u001b[33mhint: discouraged. You can squelch this message by running one of the following\u001b[m\n",
      "\u001b[33mhint: commands sometime before your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 9 (delta 7), reused 9 (delta 7), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (9/9), 2.34 KiB | 599.00 KiB/s, done.\n",
      "From https://github.com/meaningfy-ws/legal-radar\n",
      "   ab2d03d..0d9f313  master     -> origin/master\n",
      "Updating ab2d03d..0d9f313\n",
      "Fast-forward\n",
      " legal_radar/services/semantic_search_evaluation.py |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " notebooks/evaluation.ipynb                         | 114 \u001b[32m+++++++++++++++\u001b[m\u001b[31m------\u001b[m\n",
      " 2 files changed, 82 insertions(+), 34 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 11:41:40.994161: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-26 11:41:40.994188: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./legal-radar/\")\n",
    "sys.path = list(set(sys.path))\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('./legal-radar/')\n",
    "from legal_radar.services.semantic_search_evaluation import SemanticSearchEvaluation\n",
    "from legal_radar import config\n",
    "from legal_radar.services.model_registry import embedding_registry\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_BUCKET_NAME = 'faiss-index'\n",
    "FAISS_INDEX_FINREG_NAME = 'faiss_index_finreg'\n",
    "FIN_REG_SPLITTED_ES_INDEX = 'ds_finreg_splitted'\n",
    "SAMPLE_QUESTIONS_CSV_PATH = 'tests/test_data/sample_questions_v4.csv'\n",
    "SAMPLE_QUESTIONS_CSV_SEP = ';'\n",
    "RESULT_COLUMNS = [\n",
    "    'in_top_5_slices',\t'in_top_10_slices',\n",
    "    'in_top_5_documents',\t'in_top_10_documents',\t'in_q3'\n",
    "]\n",
    "EXPERIMENT_CONFIGS = [\n",
    "    (10, 5),\n",
    "    (20, 10),\n",
    "    (50, 25),\n",
    "    (100, 50)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "2021-11-26 11:41:52.467289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-26 11:41:52.467313: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-26 11:41:52.467333: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (293e399508a7): /proc/driver/nvidia/version does not exist\n",
      "2021-11-26 11:41:52.467470: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100% (5791 of 5791) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (190656 of 190656) |################| Elapsed Time: 0:01:20 Time:  0:01:20\n",
      "100% (5791 of 5791) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (94720 of 94720) |##################| Elapsed Time: 0:00:39 Time:  0:00:39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None type!\n",
      "None type!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (5791 of 5791) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (37910 of 37910) |##################| Elapsed Time: 0:00:17 Time:  0:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None type!\n",
      "None type!\n",
      "None type!\n",
      "None type!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (5791 of 5791) |####################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (19625 of 19625) |##################| Elapsed Time: 0:00:11 Time:  0:00:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None type!\n",
      "None type!\n",
      "None type!\n",
      "None type!\n",
      "None type!\n",
      "None type!\n"
     ]
    }
   ],
   "source": [
    "def compute_evaluation():\n",
    "    results = []\n",
    "    for split_window_size, split_window_step in EXPERIMENT_CONFIGS:\n",
    "        fin_reg_es_index_name = '_'.join(\n",
    "            map(str, (FIN_REG_SPLITTED_ES_INDEX, split_window_size, split_window_step)))\n",
    "        faiss_index_finreg_name = '_'.join(\n",
    "            map(str, (FAISS_INDEX_FINREG_NAME, split_window_size, split_window_step, '.pkl')))\n",
    "        semantic_search_evaluation = SemanticSearchEvaluation(documents_es_index_name=config.EU_FINREG_CELLAR_ELASTIC_SEARCH_INDEX_NAME,\n",
    "                                                              splitted_documents_es_index_name=fin_reg_es_index_name,\n",
    "                                                              faiss_bucket_name=FAISS_BUCKET_NAME,\n",
    "                                                              faiss_index_path=faiss_index_finreg_name,\n",
    "                                                              embedding_model=embedding_registry.sent2vec_universal_sent_encoding(),\n",
    "                                                              sample_questions_csv_path=SAMPLE_QUESTIONS_CSV_PATH,\n",
    "                                                              sample_questions_csv_sep=SAMPLE_QUESTIONS_CSV_SEP)\n",
    "        results.append(semantic_search_evaluation.evaluate())\n",
    "        del semantic_search_evaluation\n",
    "    result_df = []\n",
    "    for index in range(0, len(EXPERIMENT_CONFIGS)):\n",
    "        for result_columns in RESULT_COLUMNS:\n",
    "            tmp = results[index][result_columns].value_counts(normalize=True)\n",
    "            tmp_dict2 = {str(index): tmp[index] for index in tmp.index}\n",
    "            tmp_dict1 = {\n",
    "                \"split_type\": str(EXPERIMENT_CONFIGS[index]),\n",
    "                \"result_column\": result_columns,\n",
    "            }\n",
    "            tmp_dict1.update(tmp_dict2)\n",
    "            result_df.append(tmp_dict1)\n",
    "    result_df = pd.DataFrame(result_df).fillna(0)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = compute_evaluation()\n",
    "fig = px.bar(evaluation_df, x=\"split_type\", y=\"True\", color=\"result_column\",barmode='group', title=\"Search results based on window size\")\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.show()\n",
    "fig = px.bar(evaluation_df, x=\"split_type\", y=\"False\", color=\"result_column\",barmode='group', title=\"Search results based on window size\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
